---
title: Adjoint State Method
date: 2020-09-15T11:30:03+00:00
# weight: 1
# aliases: ["/first"]
tags: ["adjoint-method","optimization"]
author: "Deshana Desai, Luca Venturi"
# author: ["Me", "You"] # multiple authors
showToc: false
TocOpen: false
draft: false
hidemeta: false
comments: true
description: "The Adjoint Method is used in a wide range of application domains for solving PDE constrained optimization. These are notes on how the Adjoint Method works."
canonicalURL: "https://deshanadesai.github.io/posts/2020-06-07-Adjoint-Method/"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "<image path/url>" # image path/url
    alt: "<alt text>" # alt text
    caption: "<text>" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page
<!-- editPost:
    URL: "https://github.com/<path_to_repo>/content"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link -->
---
<!-- 
<!DOCTYPE html> -->
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Scribed by Deshana Desai, Luca Venturi" />
<!--   <title>Adjoint method</title>
 -->  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>

</div>
<div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#adjoint-method" aria-label="What is Adjoint Method">What is Adjoint Method</a><ul>
                        
                <li>
                    <a href="#a-linear-example" aria-label="A Linear Example">A Linear Example</a></li>
                <li>
                    <a href="#the-adjoint-method-for-recurrence-relations" aria-label="The Adjoint Method For Recurrence Relations">Policy Gradient</a></li>
                <li>
                    <a href="#the-non-linear-case" aria-label="The non-linear Case">The non-linear Case</a></li>
                <li>
                    <a href="#more" aria-label="More Info">More Info</a></li></ul>
                </li>
               
            </ul>
        </div>
    </details>
</div>
<body>

<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>    
<!-- <header id="title-block-header">
<h1 class="title">5 - Adjoint method</h1>
<p class="author">Scribed by Deshana Desai, Luca Venturi</p>
</header> -->

<h1 id="adjoint-method">Adjoint Method</h1>
<p>Consider we are dealing with a constrained optimization problem <span class="math display">min<sub><strong>p</strong></sub><em>g</em>(<strong>u</strong>(<strong>p</strong>),<strong>p</strong>)</span> where <span class="math inline"><strong>u</strong>(<strong>p</strong>) ∈ ℝ<sup><em>N</em></sup></span> is the solution to some equation <span class="math display"><strong>f</strong>(<strong>u</strong>(<strong>p</strong>),<strong>p</strong>) = <strong>0</strong></span> If we wish to optimize <span class="math inline"><em>g</em></span> using some gradient-type algorithm, we need an efficient way to compute the derivative of <span class="math inline"><em>g</em></span> w.r.t. <span class="math inline"><strong>p</strong> ∈ ℝ<sup><em>M</em></sup></span>: <div>
    $$
    \label{eq:g_derivative}
\frac{dg}{d{\mathbf p}} = \frac{\partial g}{\partial {\mathbf p}} + \frac{\partial g}{\partial {\mathbf u}}\frac{\partial {\mathbf u}}{\partial {\mathbf p}}
$$
</div> The adjoint method is a general procedure to reduce the computation of <span class="math inline">$\frac{dg}{d{\mathbf p}}$</span> to solving one <em>adjoint equation</em>.</p>
<h2 id="a-linear-example">A linear example</h2>
<p>Consider the case of a linear function <span class="math inline"><em>g</em></span> <span class="math display">$$\begin{aligned}
g({\mathbf u}) &amp;= {\mathbf c}^T{\mathbf u}\end{aligned}$$</span> where <span class="math inline"><strong>u</strong>(<strong>p</strong>)</span> is the solution to a (parametric) linear system <span class="math display">$$\begin{aligned}
\label{eq:linear_eq}
{\mathbf A}{\mathbf u}({\mathbf p}) = {\mathbf b}({\mathbf p})\end{aligned}$$</span> where <span class="math inline"><strong>b</strong> : ℝ<sup><em>M</em></sup> → ℝ<sup><em>N</em></sup></span> and <span class="math inline"><strong>A</strong> ∈ ℝ<sup><em>N</em> × <em>N</em></sup></span> is a fixed matrix. <span class="math inline"><strong>u</strong>(<strong>p</strong>)</span> is the solution to the system and is thus given by <span class="math inline"><strong>u</strong>(<strong>p</strong>) = <strong>A</strong><sup> − 1</sup><strong>b</strong>(<strong>p</strong>)</span>. If we derive <span class="math inline"><strong>u</strong></span> w.r.t. to <span class="math inline"><strong>p</strong></span>, we get <span class="math display">$$\begin{aligned}
\frac{\partial {\mathbf u}}{\partial {\mathbf p}} = {\mathbf A}^{-1} \frac{\partial {\mathbf b}}{\partial {\mathbf p}}\end{aligned}$$</span> Plugging this into equation <a href="#eq:g_derivative" data-reference-type="eqref" data-reference="eq:g_derivative">[eq:g_derivative]</a>, we get <span class="math display">$$\begin{aligned}
\frac{d g}{d {\mathbf p}} &amp;= \frac{\partial g}{\partial {\mathbf u}}\frac{\partial {\mathbf u}}{\partial {\mathbf p}} = {\mathbf c}^T{\mathbf A}^{-1}\frac{\partial {\mathbf b}}{\partial {\mathbf p}}\label{eq:adj_linear}\end{aligned}$$</span> Recall that <span class="math inline"><strong>c</strong></span> is a <span class="math inline"><em>N</em></span> dimensional vector, <span class="math inline"><strong>A</strong></span> is a <span class="math inline"><em>N</em> × <em>N</em></span> matrix, and <span class="math inline">$\frac{\partial {\mathbf b}}{\partial {\mathbf p}}$</span> is a <span class="math inline"><em>N</em> × <em>M</em></span> matrix. There are two possible ways to compute the matrix product in the RHS of equation <a href="#eq:adj_linear" data-reference-type="eqref" data-reference="eq:adj_linear">[eq:adj_linear]</a>:</p>
<ul>
<li><p><span class="math inline">$({\mathbf c}^T{\mathbf A}^{-1})\frac{\partial {\mathbf b}}{\partial {\mathbf p}}$</span>. This results in a computational cost of <span class="math inline"><em>O</em>(<em>N</em>(<em>N</em>+<em>M</em>))</span>.</p></li>
<li><p><span class="math inline">${\mathbf c}^T({\mathbf A}^{-1}\frac{\partial {\mathbf b}}{\partial {\mathbf p}})$</span>. This results in a computational cost of <span class="math inline"><em>O</em>(<em>N</em><em>M</em>(1+<em>N</em>))</span>.</p></li>
</ul>
<p>Clearly, the first method is more efficient as <span class="math inline"><em>M</em> &gt; 1</span>. Summarizing, we can re-formulate the above as <span class="math display">$$\frac{\partial g}{\partial {\mathbf p}} = {\boldsymbol \lambda}^T \frac{\partial {\mathbf b}}{\partial {\mathbf p}}$$</span> where <span class="math inline"><strong>λ</strong></span> is the solution to <span class="math display"><strong>A</strong><sup><em>T</em></sup><strong>λ</strong> = <strong>c</strong></span> This is basically the main idea of the adjoint method. Notice that, in fact, we moved to solving a linear system whose matrix is the adjoint of the original linear system <a href="#eq:linear_eq" data-reference-type="eqref" data-reference="eq:linear_eq">[eq:linear_eq]</a>. In the following, we see how to apply the same procedure to more involved settings.</p>
<h2 id="the-non-linear-case">The non-linear case</h2>
<p>Consider a generic non-linear function <span class="math inline"><em>g</em> : ℝ<sup><em>N</em></sup> × ℝ<sup><em>M</em></sup> → ℝ</span> and assume that <span class="math inline"><strong>u</strong>(<strong>p</strong>)</span> is taken to be the solution to a generic non-linear system <span class="math display">$$\begin{aligned}
{\mathbf f}({\mathbf u}({\mathbf p}),{\mathbf p}) = {\mathbf 0}\end{aligned}$$</span> for some <span class="math inline"><strong>f</strong> : ℝ<sup><em>N</em></sup> × ℝ<sup><em>M</em></sup> → ℝ<sup><em>N</em></sup></span>. Deriving the above equation w.r.t. <span class="math inline"><strong>p</strong></span>, we get <span class="math display">$$\begin{aligned}
0 &amp;= \frac{d}{d{\mathbf p}} {\mathbf f}({\mathbf u}({\mathbf p}),{\mathbf p}) = \frac{\partial {\mathbf f}}{\partial {\mathbf u}}\frac{\partial {\mathbf u}}{\partial {\mathbf p}} + \frac{\partial {\mathbf f}}{\partial {\mathbf p}} \end{aligned}$$</span> Solving for <span class="math inline">$\frac{\partial {\mathbf u}}{\partial {\mathbf p}}$</span>, we get <span class="math display">$$\begin{aligned}
\frac{\partial {\mathbf u}}{\partial {\mathbf p}} = -\left( \frac{\partial {\mathbf f}}{\partial {\mathbf u}}\right)^{-1}\left( \frac{\partial {\mathbf f}}{\partial {\mathbf p}} \right)\end{aligned}$$</span> Plugging this into <a href="#eq:g_derivative" data-reference-type="eqref" data-reference="eq:g_derivative">[eq:g_derivative]</a>, we get <span class="math display">$$\begin{aligned}
\frac{d g}{d {\mathbf p}} &amp;= \frac{\partial g}{\partial {\mathbf p}} + \frac{\partial g}{\partial {\mathbf u}}\frac{\partial {\mathbf u}}{\partial {\mathbf p}} = \frac{\partial g}{\partial {\mathbf p}} - \frac{\partial g}{\partial {\mathbf u}}\left( \frac{\partial {\mathbf f}}{\partial {\mathbf u}}\right)^{-1}\left( \frac{\partial {\mathbf f}}{\partial {\mathbf p}} \right)\end{aligned}$$</span> As before, the efficient way to evaluate the above term is by first computing the product <span class="math inline">$\frac{\partial g}{\partial {\mathbf u}}\left( \frac{\partial {\mathbf f}}{\partial {\mathbf u}}\right)^{-1}$</span> and then multiply the resultant matrix with <span class="math inline">$\frac{\partial {\mathbf f}}{\partial {\mathbf p}}$</span>. Equivalently, we can define the adjoint <span class="math inline"><strong>λ</strong><sup><em>T</em></sup></span> as the solution to the equation <span class="math display">$$\begin{aligned}
\left( \frac{\partial {\mathbf f}}{\partial {\mathbf u}} \right)^T {\boldsymbol \lambda}&amp;= - \left( \frac{\partial g}{\partial {\mathbf u}} \right)^T \end{aligned}$$</span> and take <span class="math display">$$\frac{d g}{d {\mathbf p}} = \frac{\partial g}{\partial {\mathbf p}} - {\boldsymbol \lambda}^T\left( \frac{\partial {\mathbf f}}{\partial {\mathbf p}} \right)$$</span></p>
<h2 id="the-adjoint-method-for-recurrence-relations">The adjoint method for recurrence relations</h2>
<p>Assume we wish to evaluate <span class="math inline">$\frac{\partial g}{\partial {\mathbf p}}$</span>, where <span class="math inline"><em>g</em></span> is given by <span class="math display"><em>g</em> = <em>g</em><sup><em>n</em></sup> ≐ <em>g</em>(<strong>x</strong><sup><em>n</em></sup>(<strong>p</strong>),<strong>p</strong>)</span> where <span class="math inline"><strong>x</strong><sup>0</sup> = <strong>b</strong>(<strong>p</strong>)</span> and <span class="math inline"><strong>x</strong><sup><em>n</em></sup></span> is defined by the iteration <span class="math display"><strong>x</strong><sup><em>k</em></sup> = <strong>f</strong><sup><em>k</em></sup> ≐ <strong>f</strong>(<em>k</em>,<strong>x</strong><sup><em>k</em> − 1</sup>,<strong>p</strong>)</span> The derivative <span class="math inline">$\frac{\partial g}{\partial {\mathbf p}}$</span> reads <span class="math display">$$\begin{aligned}
\frac{dg}{d{\mathbf p}} = \frac{\partial g^n}{\partial {\mathbf x}}\frac{\partial {\mathbf x}^n}{\partial {\mathbf p}} + \frac{\partial g^n}{\partial {\mathbf p}}\end{aligned}$$</span> and taking the derivative of <a href="#eq:iteration" data-reference-type="eqref" data-reference="eq:iteration">[eq:iteration]</a> w.r.t. <span class="math inline"><strong>p</strong></span> we get <span class="math display">$$\frac{\partial {\mathbf x}^k}{\partial {\mathbf p}} = \frac{\partial {\mathbf f}^k}{\partial {\mathbf x}} \frac{\partial {\mathbf x}^{k-1}}{\partial {\mathbf p}} + \frac{\partial {\mathbf f}^k}{\partial {\mathbf p}} = {\mathbf f}_{\mathbf x}^k{\mathbf x}_{\mathbf p}^{k-1} + {\mathbf f}^k_{\mathbf p}$$</span> where we used the notations <span class="math inline">$\frac{\partial {\mathbf f}^k}{\partial {\mathbf x}} = {\mathbf f}_{\mathbf x}^k$</span> and similar for other variables. Putting the two equations together, we get <span class="math display">$$\begin{aligned}
\frac{dg}{d{\mathbf p}} &amp;= g_{\mathbf x}^n {\mathbf x}_{\mathbf p}^n + g_{\mathbf p}^n \nonumber\\
&amp;= g_{\mathbf x}^n \left( {\mathbf f}_{\mathbf x}^n {\mathbf x}_{\mathbf p}^{n-1} + {\mathbf f}_{\mathbf p}^n \right) + g_{\mathbf p}^n\nonumber \\
&amp;= g_{\mathbf p}^n \left( {\mathbf f}_{\mathbf x}^n \left( {\mathbf f}_{\mathbf x}^{n-1}{\mathbf x}_{\mathbf p}^{n-2} + {\mathbf f}_{\mathbf p}^{n-1} \right) + {\mathbf f}_{\mathbf p}^n\right) + g_{\mathbf p}^n\nonumber \\
&amp;= g_{\mathbf x}^n \left( {\mathbf f}_{\mathbf x}^n {\mathbf f}_{\mathbf x}^{n-1} {\mathbf x}_{\mathbf p}^{n-2} + {\mathbf f}_{\mathbf x}^{n} {\mathbf f}_{\mathbf p}^{n-1} +{\mathbf f}_{\mathbf p}^{n} \right) + g_{\mathbf p}^n \nonumber\\
&amp;= \cdots \nonumber\\ 
&amp; = g_{\mathbf p}^n + g_{\mathbf x}^n\left( \left(\prod_{i=n}^1 {\mathbf f}_{\mathbf x}^i\right) {\mathbf b}_{\mathbf p}+ \sum_{i=1}^n \left( \prod_{j=n}^{i+1} {\mathbf f}_{\mathbf x}^j \right) {\mathbf f}_{\mathbf p}^i \right) \label{eq:iter_ineff}\end{aligned}$$</span> Equivalently, one can define <span class="math inline">${\boldsymbol \lambda}^k = g_{\mathbf x}^n\left(
\prod_{i=n}^{k+1} {\mathbf f}_{\mathbf x}^i\right)$</span>. Then <span class="math inline"><strong>λ</strong><sup><em>k</em></sup></span> verifies the (backward) recurrent relation <span class="math display">$$\begin{aligned}
{\boldsymbol \lambda}^n &amp;= g^n_{\mathbf x}\nonumber \\
{\boldsymbol \lambda}^{k} &amp;= {\boldsymbol \lambda}^{k+1}{\mathbf f}^{k+1}_x  \label{eq:adjoint_iter}\end{aligned}$$</span> and <span class="math inline">$\frac{dg}{d{\mathbf p}}$</span> can be evaluated as <span class="math display">$$\frac{dg}{d{\mathbf p}} = g_{\mathbf p}^n + {\boldsymbol \lambda}^0{\mathbf b}_{\mathbf p}+ \sum_{i=1}^n {\boldsymbol \lambda}^i {\mathbf f}^i_{\mathbf p}$$</span> In this case, the adjoint equation <a href="#eq:adjoint_iter" data-reference-type="eqref" data-reference="eq:adjoint_iter">[eq:adjoint_iter]</a> is therefore given by a backward recurrent relation.</p>
<h4 id="question">Question:</h4>
<p>Compute the computational cost of evaluating <span class="math inline">$\frac{dg}{d{\mathbf p}}$</span> using formula <a href="#eq:iter_ineff" data-reference-type="eqref" data-reference="eq:iter_ineff">[eq:iter_ineff]</a> (naively) and using the adjoint method. Conclude that the adjoint method is more efficient.</p>
<h2 id="ode-constraints">ODE constraints</h2>
<p>We now turn to the real reason why we are looking into the adjoint method: evaluate <span class="math inline">$\frac{dg}{d{\mathbf p}}$</span> where <span class="math inline"><em>g</em></span> is given by <span class="math display"><em>g</em> = <em>g</em><sup><em>T</em></sup> ≐ <em>g</em>(<strong>x</strong><sup><em>τ</em></sup>(<strong>p</strong>),<strong>p</strong>)</span> and <span class="math inline"><strong>x</strong><sup><em>τ</em></sup></span> is the solution to the initial value problem <span class="math display">$$\begin{aligned}
\dot{{\mathbf x}}^t &amp; = {\mathbf f}(t,{\mathbf x}^t,{\mathbf p})\\
{\mathbf x}^0 &amp; = {\mathbf b}({\mathbf p})\end{aligned}$$</span> at time <span class="math inline"><em>t</em> = <em>τ</em></span>. In the following, we use the notation <span class="math inline"><strong>x</strong><sub><strong>p</strong></sub><sup><em>t</em></sup></span> for <span class="math inline">$\frac{\partial {\mathbf x}^t}{\partial {\mathbf p}}$</span> and similar.</p>
<h4 id="the-linear-case">The linear case</h4>
<p>Let’s first consider the easier case where <span class="math inline"><strong>f</strong></span> is a linear function: <span class="math display">$$\begin{aligned}
{\mathbf x}(0) &amp;= {\mathbf b}({\mathbf p}) \\
\dot{{\mathbf x}}^t &amp;={\mathbf A}{\mathbf x}^t\end{aligned}$$</span> The solution is given by <span class="math inline"><strong>x</strong><sup><em>t</em></sup> = <em>e</em><sup><em>t</em><strong>A</strong></sup><strong>b</strong>(<strong>p</strong>)</span>. Plugging this into <a href="#eq:g_derivative" data-reference-type="eqref" data-reference="eq:g_derivative">[eq:g_derivative]</a>, we get <span class="math display">$$\begin{aligned}
\frac{dg}{d{\mathbf p}} = g^\tau_{\mathbf x}e^{\tau {\mathbf A}}{\mathbf b}_{\mathbf p}\end{aligned}$$</span> Similarly to the linear case, we can call <span class="math inline"><strong>λ</strong><sup><em>T</em></sup> = <em>g</em><sub><strong>x</strong></sub><sup><em>τ</em></sup><em>e</em><sup><em>τ</em><strong>A</strong></sup></span> so that <span class="math inline">$\frac{dg}{d{\mathbf p}} = {\boldsymbol \lambda}^T{\mathbf b}_{\mathbf p}$</span> Notice that now <span class="math inline"><strong>λ</strong> = <strong>λ</strong><sup>0</sup></span>, where <span class="math inline"><strong>λ</strong><sup><em>t</em></sup></span> solves the (backward in time) ODE <span class="math display">$$\begin{aligned}
\dot{{\boldsymbol \lambda}^t} &amp; = -{\mathbf A}^T{\boldsymbol \lambda}^t \\    
{\boldsymbol \lambda}^\tau &amp; = (g^\tau_{\mathbf x})^T\end{aligned}$$</span> The adjoint equation is therefore given by backward ODE. We can easily see similarity with the case of a recurrence relation.</p>
<h4 id="the-non-linear-case-1">The non-linear case</h4>
<p>It was easy to derive the adjoint equation for the case of a linear ODE. What about a more general ODE? For simplicity, consider the case of <span class="math inline"><em>g</em></span> given by <span class="math display"><em>g</em> = <em>g</em><sup><em>τ</em></sup> = <em>g</em>(<strong>x</strong><sup><em>τ</em></sup>)</span> where <span class="math inline"><strong>x</strong><sup><em>t</em></sup></span> solves <span class="math display">$$\begin{aligned}
\dot{{\mathbf x}}^t &amp;= {\mathbf f}({\mathbf x}) \\
{\mathbf x}^0 &amp;= {\mathbf b}({\mathbf p})\end{aligned}$$</span> In this case, it’s not that trivial anymore to find <span class="math inline"><strong>x</strong><sub><strong>p</strong></sub><sup><em>τ</em></sup></span>. Motivated by the above linear case, we can think to consider a linearisation of the ODE: <span class="math display">$$\dot{{\mathbf x}}_{\mathbf p}^t = {\mathbf f}_{\mathbf x}{\mathbf x}^t_{\mathbf p}$$</span> where <span class="math inline"><strong>f</strong><sub><strong>x</strong></sub> = <strong>f</strong><sub><strong>x</strong></sub><sup><em>t</em></sup></span>, which we are assuming here to be well approximated by a constant matrix. As we saw above, the adjoint equation is given by <span class="math display">$$\dot{{\boldsymbol \lambda}}^t = - ({\mathbf f}_{\mathbf x})^T {\boldsymbol \lambda}^t$$</span> and <span class="math inline">$\frac{d g}{d{\mathbf p}} = ({\boldsymbol \lambda}^0)^T {\mathbf x}^0_{\mathbf p}$</span>. In the general case, simply by analogy, we could try to define the adjoint ODE by <span class="math display">$$\begin{aligned}
\dot{{\boldsymbol \lambda}}^t &amp; = - ({\mathbf f}_{\mathbf x}^t)^T {\boldsymbol \lambda}^t\\
{\boldsymbol \lambda}^\tau &amp;= (g^\tau_{\mathbf x})^T\end{aligned}$$</span> Turns out that this choice is actually correct, in the following sense. We know, by definition, that <span class="math display">$$\frac{dg}{d{\mathbf p}} = ({\boldsymbol \lambda}^\tau)^T{\mathbf x}^\tau_{\mathbf p}$$</span> Moreover, the solution <span class="math inline"><strong>λ</strong><sup><em>t</em></sup></span> to the above ODE satisfies <span class="math display">$$\parr*{\parr*{{\boldsymbol \lambda}^t}^T{\mathbf x}^t_{\mathbf p}}' = \parr*{\dot{{\boldsymbol \lambda}^t}}^T{\mathbf x}^t_{\mathbf p}+ \parr*{{\boldsymbol \lambda}^t}^T\dot{{\mathbf x}}^t_{\mathbf p}= -\parr*{{\boldsymbol \lambda}^t}^T {\mathbf f}_{\mathbf x}^t {\mathbf x}^t_{\mathbf p}+ \parr*{{\boldsymbol \lambda}^t}^T {\mathbf f}_{\mathbf x}^t {\mathbf x}^t_{\mathbf p}= {\mathbf 0}$$</span> Therefore <span class="math inline">$\parr*{{\boldsymbol \lambda}^t}^T{\mathbf x}_{\mathbf p}^t \equiv \parr*{{\boldsymbol \lambda}^0}^T{\mathbf x}_{\mathbf p}^0$</span> is constant and thus <span class="math display">$$\frac{dg}{d{\mathbf p}} = \parr*{{\boldsymbol \lambda}^\tau}^T{\mathbf x}^\tau_{\mathbf p}= \parr*{{\boldsymbol \lambda}^0}^T{\mathbf x}^0_{\mathbf p}= \parr*{{\boldsymbol \lambda}^0}^T{\mathbf b}_{\mathbf p}$$</span> which matches the formula for the case of a linear ODE. A computation similar to the previous considered cases proves the greater efficiency of this formula.</p>
<h2 id="more">More</h2>
<p>The adjoint method can be generalized, in a very similar fashion, to more involved problems. For an introduction, we refer to <span class="citation" data-cites="strang2007computational"></span>. One could also notice some affinities with backward automatic differentiation, where the use of the chain rule in a backward fashion allows to compute gradients more efficiently.</p>
<p>Consider the problem of optimizing a real-valued function <span class="math inline"><em>g</em></span> over the solution of the ODE <span class="math inline"><em>y</em>′(<em>t</em>) = <em>A</em>(<em>p</em>)<em>y</em>(<em>t</em>)</span>, <span class="math inline"><em>y</em>(0) = <em>b</em>(<em>p</em>)</span> at time <span class="math inline"><em>τ</em> &gt; 0</span>: <span class="math inline">min<sub><em>p</em></sub> <em>g</em>(<em>T</em>) ≐ <em>g</em>(<em>y</em>(<em>T</em>;<em>p</em>))</span>. Find <span class="math inline">$\frac{dg}{dp}(T)$</span> by solving the ODE and by applying chain rule. Check the correctness of equations (16-17) in *CSE*.</p>
<p>It holds that</p>
<p><span class="math display"><em>y</em>(<em>t</em>) = <em>e</em><sup><em>t</em><em>A</em>(<em>p</em>)</sup><em>y</em>(0)</span></p>
<p>Applying the chain rule, we get</p>
<p><span class="math display">$$\frac{dg}{dp} = \frac{dg}{dy}e^{TA(p)}\frac{db}{dp} + T\frac{dg}{dy}\frac{\partial A}{\partial p}e^{TA(p)}b(p)$$</span></p>
<p>On the other hand, the adjoint ODE reads</p>
<p><span class="math display"><em>λ</em>′(<em>t</em>) =  − <em>A</em>(<em>p</em>)<sup><em>T</em></sup><em>λ</em>(<em>t</em>)</span></p>
<p>with the final condition <span class="math inline">$\lambda(T) = \left(\frac{\partial g}{\partial y}\right)^T$</span>, which gives <span class="math inline">$\lambda(t) = e^{A(p)^T(T-t)}\left(\frac{\partial g}{\partial y}\right)^T$</span>. Equation (17) from *CSE* gives</p>
<p><span class="math display">$$\frac{dg}{dp} = \left(e^{TA(p)^T}\left(\frac{\partial g}{\partial y}\right)^T\right)^T\frac{\partial b}{\partial p} + \int_0^T \frac{\partial g}{\partial y} e^{A(p)(T-t)}\frac{\partial A}{\partial p}e^{tA(p)}b(p)\,dt$$</span></p>
<p>which coincides with the above.</p>
<p>By definition, it holds that</p>
<p><span class="math display">$$\frac{dG}{dp} = \int_0^T\left(\frac{\partial g}{\partial p} + \frac{\partial g}{\partial u}\frac{\partial u}{\partial p}\right)\,dt$$</span></p>
<p>On the other hand, it holds that</p>
<p><span class="math display">$$\lambda(0)^T\frac{\partial u}{\partial p}(0) + \int_0^T\lambda^T \frac{\partial f}{\partial p}\,dt = \int_0^T \left( \lambda^T\frac{\partial f}{\partial p} -\frac{d}{dt}\left( \lambda^T \frac{\partial u}{\partial p}\right) \right)\,dt$$</span></p>
<p>Using equation (14) from *CSE* and the equality <span class="math inline">$\frac{\partial u}{\partial p} = \frac{\partial f}{\partial p} + \frac{\partial f}{\partial u}\frac{\partial u}{\partial p}$</span>, we get</p>
<p><span class="math display">$$\int_0^T \left( \lambda^T\frac{\partial f}{\partial p} -\frac{d}{dt}\left( \lambda^T \frac{\partial u}{\partial p}\right) \right)\,dt = \int_0^T \left( \lambda^T\frac{\partial f}{\partial p} + \lambda^T \frac{\partial f}{\partial u}\frac{\partial u}{\partial p} + \frac{\partial g}{\partial u}\frac{\partial u}{\partial p} - \lambda^T \frac{\partial f}{\partial p} -\lambda^T \frac{\partial f}{\partial u}\frac{\partial u}{\partial p} \right)\,dt$$</span></p>
<p>which gives</p>
<p><span class="math display">$$\lambda(0)^T\frac{\partial u}{\partial p}(0) + \int_0^T \lambda^T\frac{\partial f}{\partial p}\,dt = \int_0^T \frac{\partial g}{\partial u}\frac{\partial u}{\partial p}\,dt$$</span></p>
<p>and thus completes the proof.</p>
</body>
<!-- </html> -->
